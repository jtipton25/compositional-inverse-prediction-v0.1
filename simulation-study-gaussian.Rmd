---
title: "Multivariate Gaussian Process"
author: "John Tipton"
date: "\today"
output: html_document  
---


```{r setup, tidy=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=6, include=FALSE, eval=TRUE}
set.seed(101)
library(knitr)
library(mvnfast)
library(ggplot2)
library(Matrix)
library(fields)
library(MCMCpack)
library(coda)
library(snowfall)
library(parallel)
library(rlecuyer)
library(xtable)
library(reshape2)
library(data.table)
library(reshape2)
library(randomForest)
library(GGally)
library(gridExtra)
library(here)

##
## source functions used in the code below
##

## simulate the cholesky of the correlation matrix
source(here("functions", "make-lkj.R"))

## simulate the correlation matrix
source(here("functions", "make-correlation-matrix.R"))

## layout multiple ggplot objects in one plot
source(here("functions", "multiplot.R"))

## load function to convert MCMC output to CODA object for diagnostics
source(here("functions", "convert-to-coda.R"))

## load Gelman-Rubin diagnostic for lack of convergence
source(here("functions", "make_gelman_rubin.R"))

## evaluate a probabilistic predictive distribution using CRPS score
Rcpp::sourceCpp(here("functions", "makeCRPS.cpp"))

## evaluate a Gaussian predictive distribution summarized by mean and 
## standard deviation using CRPS score
source(here("functions", "makeCRPSGauss.R"))


##
## Setup MCMC parameters
##

n_adapt <- 150000
n_mcmc <- 150000
n_thin <- 100
n_chains <- 4
## B-spline degrees of freedom parameter
df <- 6
degree <- 3
n_save <- n_mcmc / n_thin
n_samples <- n_chains * n_save
## Output MCMC progress modlulo every 'message' iterations
message <- 1000

## currently can choose exponential or gaussian covariance function
corr_function <- "exponential"
```

```{r simulation, tidy=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=6, include=FALSE, eval=TRUE}
set.seed(141)
## number of sites from which to simulate
N <- 1000

## number of hold-out sites to use for reconstruction
N_obs <- N - 200
## index for the training dataset
train <- 1:N_obs
## index for the test dataset
test <- (N_obs+1):N
## number of "species"
d <- 8

corr_out <- make_correlation_matrix(d, eta = 1)
R <- corr_out$R
xi <- corr_out$xi

## simulate the marginal variances
tau <- rgamma(d, 5, 5)
R_tau <- R %*% diag(tau)
Sigma <- t(R_tau) %*% R_tau

mu <- rnorm(d, 0, 10)
X <- rnorm(N, 0, 1)
X_train <- X
X_train[test] <- NA
sigma2 <- 1
phi <- 1.25
epsilon <- matrix(rnorm(N*d, 0, sqrt(sigma2)), N, d)

##
## simulate from the full model
##
# D <- as.matrix(dist(X))
# C <- exp(- D / phi)

# zeta <- t(rmvn(d, rep(0, N), C)) %*% R

##
## Simulate frome the predictive process model
## 

n_knots <- 30
X_knots <- seq(min(X), max(X), length=n_knots)
D_knots <- as.matrix(dist(X_knots))               ## distance among knots
D_interpolate <- as.matrix(rdist(X, X_knots))     ## distance from observed 
                                                  ## locations to knots
C_knots <- exp(- D_knots / phi)
C_knots_inv <- solve(C_knots)
c_knots <- exp( - D_interpolate / phi)
Z_knots <- c_knots %*% C_knots_inv
zero_knots <- rep(0, n_knots)
eta_star <- t(rmvn(d, zero_knots, chol(C_knots), isChol=TRUE))
zeta <- Z_knots %*% eta_star %*% R_tau

y <- t(mu + t(zeta)) + epsilon
```


```{r}
simPlotData <- data.frame(species=as.factor(rep(1:d, each=N)), 
                          response=c(y), 
                          covariate=rep(X, times=d), 
                          zeta=c(t(mu + t(zeta))))

gsim1 <- ggplot(simPlotData, aes(x=covariate, y=response, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("Gaussian MVGP") + 
  geom_line(aes(x=covariate, y=zeta, col = species), simPlotData, lwd=1)

gsim2 <- ggplot(simPlotData, aes(x=covariate, y=response, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("Gaussian MVGP by dimension") + 
  geom_line(aes(x=covariate, y=zeta, col = species), simPlotData, lwd=1) + 
  facet_wrap( ~ species, ncol = 2) + 
  theme(axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

multiplot(gsim1, gsim2, cols=2)
```


## Fit the Gaussian Likelihood model
This model took 1.21 hours running 4 mcmc chains in parallel on a 2017 iMac with 4.2GHz processor.

```{r fitSim, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
if (file.exists(here("model-fit", "fitSim.RData"))) {
  ## Load MCMC run
  load(here("model-fit", "fitSim.RData"))
} else {
  
  ## 
  ## Long running MCMC 
  ##
  
  ## Define parameters 
  params <- list(n_adapt=n_adapt, n_mcmc=n_mcmc,
                 n_thin=n_thin, N_obs=N_obs, X_knots=X_knots, 
                 message=message)
  
  parallelChains <- function (n_chains) {
    Rcpp::sourceCpp(here("mcmc", "mcmc-mvgp.cpp"))
    out <- mcmc(mcmcRcpp(y, X, params, n_chain=n_chains, 
                         file_name=here("model-fit", "progress", "sim-fit.txt"), 
                         corr_function="exponential"))
  }
  
  ## Initalize multicore
  sfInit(parallel=TRUE, cpus=4)
  sfClusterSetupRNG()
  sfExport("y", "X", "params")
  sfLibrary(coda)
  sfLibrary(here)
  
  ## create temporary progress file and start timing
  file.create(here("model-fit", "progress", "sim-fit.txt"))
    start <- Sys.time()
  sink(here("model-fit", "progress", "sim-fit.txt"))
  print(paste("MCMC started at", start))
  sink()

  ## run MCMC
  out <- sfLapply(1:4, parallelChains)
  
  ## end timing
  sink(here("model-fit", "progress", "sim-fit.txt"), append = TRUE)
  print(Sys.time() - start)
  sink()
  
  ## stop the computing cluster
  sfStop()

  save(out, X_knots, file=here("model-fit", "fitSim.RData"))
}

```


```{r}
## initialze posterior sample variables
mu_post <- matrix(0, n_samples, d)
eta_star_post <- array(0, dim=c(n_samples, n_knots, d))
zeta_post <-  array(0, dim=c(n_samples, N, d))
phi_post <- rep(0,  n_samples)
sigma2_post <- rep(0,  n_samples)
tau2_post <- matrix(0,  n_samples, d)
X_post <- matrix(0,  n_samples, N-N_obs)
xi_post <- array(0,  dim=c(n_samples, choose(d, 2)))
Omega_post <- array(0, dim=c(n_samples, d, d))
R_post <- array(0, dim=c(n_samples, d, d))

for(i in 1:n_chains){
    mu_post[1:n_save + (i-1)*n_save, ] <- out[[i]]$mu
    eta_star_post[1:n_save + (i-1)*n_save, , ] <- out[[i]]$eta_star
    zeta_post[1:n_save + (i-1)*n_save, , ] <- out[[i]]$zeta
    phi_post[1:n_save + (i-1)*n_save] <- out[[i]]$phi
    sigma2_post[1:n_save + (i-1)*n_save] <- out[[i]]$sigma2
    tau2_post[1:n_save + (i-1)*n_save, ] <- out[[i]]$tau2
    X_post[1:n_save + (i-1)*n_save, ] <- out[[i]]$X
    xi_post[1:n_save + (i-1)*n_save, ] <- out[[i]]$xi
    R_post[1:n_save + (i-1)*n_save, , ] <- out[[i]]$R
    Omega_post[1:n_save + (i-1)*n_save, , ] <- out[[i]]$Omega
}          
```



```{r, eval=TRUE}
out <- convert_to_coda(out)
```



```{r}
Rhat <- make_gelman_rubin(out)
layout(matrix(1:9, 3, 3))
hist(Rhat[grepl("mu", names(Rhat))], main = "Rhat for mu")
hist(Rhat[grepl("eta_star", names(Rhat))], main = "Rhat for eta_star")
hist(Rhat[grepl("zeta", names(Rhat))], main = "Rhat for zeta")
hist(Rhat[grepl("tau2", names(Rhat))], main = "Rhat for tau2")
plot(Rhat[grepl("phi", names(Rhat))], main = "Rhat for phi")
abline(h=1, col='red')
hist(Rhat[grepl("xi", names(Rhat))], main = "Rhat for xi")
hist(Rhat, main="All parameters")
Rhat[grepl("phi", names(Rhat))]
max(unlist(na.omit(Rhat)))
```


```{r, eval=TRUE}
## 
## Posterior plots
##

layout(matrix(1:9, 3, 3))
matplot(mu_post, type = 'l')
abline(h=mu, col='red', lwd=2)
plot(phi_post, type='l')
abline(h=phi, col='red', lwd=2)
plot(sigma2_post, type='l')
abline(h=sigma2, col='red', lwd=2)
matplot(tau2_post, type='l')
abline(h=tau^2)
matplot(xi_post, type='l')
abline(h=xi, col='red')
matplot(eta_star_post[, , 1], type='l')
# abline(h=eta_star[, 1], col='red', lwd=2)
matplot(eta_star_post[, , 2], type='l')
# abline(h=eta_star[, 2], col='red', lwd=2)
matplot(eta_star_post[, , 3], type='l')
# abline(h=eta_star[, 3], col='red', lwd=2)
# matplot(eta_star_post[, , 4], type='l')
# abline(h=eta_star[, 4], col='red', lwd=2)
matplot(X_post, type='l')
abline(h=X[(N_obs+1):N])
```




```{r, eval=TRUE}
layout(matrix(1:4, 2, 2))
matplot(zeta_post[, 1, ], type='l')
abline(h=zeta[1, ], col='red', lwd=2)
matplot(zeta_post[, 2, ], type='l')
abline(h=zeta[2, ], col='red', lwd=2)
matplot(zeta_post[, 3, ], type='l')
abline(h=zeta[3, ], col='red', lwd=2)
matplot(zeta_post[, 4, ], type='l')
abline(h=zeta[4, ], col='red', lwd=2)
```


```{r, eval=TRUE}
layout(matrix(1:4, 2, 2))
matplot(R_post[, , 1], type='l')
abline(h=R[, 1])
matplot(R_post[, , 2], type='l')
abline(h=R[, 2])
matplot(R_post[, , 3], type='l')
abline(h=R[, 3])
matplot(R_post[, , 4], type='l')
abline(h=R[, 4])
```





```{r, eval=TRUE}
layout(matrix(1:3, 3, 1))
idx <- order(X)
zeta_post_mean <- apply(zeta_post, c(2,3), mean)
matplot(X[idx], zeta_post_mean[idx, ], type='l')
matplot(X[idx], zeta[idx, ], type = 'l')
matplot(X[idx], zeta_post_mean[idx, ] - zeta[idx, ], type = 'l')
```



```{r, eval=TRUE}
Omega_post_mean <- matrix(0, d, d)
for (i in 1:n_samples) {
  Omega_post_mean <- Omega_post_mean + 
    1/n_samples * t(R_post[i, , ]) %*% R_post[i, , ]
}

multiplot(ggcorr(data=NULL, cor_matrix=cov2cor(Sigma)) + 
            ggtitle("Simulated Correlations"), 
          ggcorr(data=NULL, cor_matrix=Omega_post_mean) + 
            ggtitle("Posterior Mean Correlations"),
          ggcorr(data=NULL, cor_matrix=Omega_post_mean-cov2cor(Sigma)) + 
            ggtitle("Estimation Error for Correlations"),
          cols=1)
```



```{r, eval=FALSE}
mu_post_mean <- apply(mu_post, 2, mean)
zeta_post_mean <- apply(zeta_post, c(2, 3), mean)
p_alpha <- matrix(0, N, d)
for (i in 1:N) {
  p_alpha[i, ] <- mu_post_mean + zeta_post_mean[i, ]
}

fitPlotData <- data.frame(species=as.factor(rep(1:d, each=N_obs)), count=c(y[1:N_obs, ]), 
                          depth=rep(X[1:N_obs], times=d), alpha=c(p_alpha[1:N_obs, ]))

g1_post <- ggplot(fitPlotData, aes(x=depth, y=count, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("Estimated Functional response vs.covariate") + 
  geom_line(aes(x=depth, y=alpha, col = species), fitPlotData, lwd=1.25)

g2_post <- ggplot(fitPlotData, aes(x=depth, y=count, color=species, group=species)) + 
  geom_point(alpha=0.25) + theme(legend.position="none") +
  ggtitle("Estimated Functional response vs.covariateby species") + 
  geom_line(aes(x=depth, y=alpha, col = species), fitPlotData, lwd=1.25) + 
  facet_wrap( ~ species, ncol = 2)

multiplot(g1_post, g2_post, gsim1, gsim2, cols=2)
```




## Fit the Gaussian Likelihood B-spline model
This model took 15.6 minutes running 4 mcmc chains in parallel on a 2017 iMac with 4.2GHz processor.

```{r fitSplineSim, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, eval=TRUE}
if (file.exists(here("model-fit", "fit-gaussian-gam.RData"))) {
  ## Load MCMC run
  load(here("model-fit", "fit-gaussian-gam.RData"))
} else {
  ##
  ## Long running MCMC
  ##

  ## Define parameters
  params <- list(n_adapt=n_adapt, n_mcmc=n_mcmc, df=df,
               degree=degree, N_obs=N_obs, message=message,
               n_thin=n_thin)

  ## Fit MCMC model
  parallelChains <- function (n_chains) {
    # Rcpp::sourceCpp('~/mvgp/mcmc/mcmc-basis-missing-covariate-ess.cpp')
    Rcpp::sourceCpp(here("mcmc", "mcmc-gam.cpp"))
    out <- mcmc( mcmcRcpp(y, X, params, n_chain=n_chains,
                          file_name=here("model-fit", "progress", "sim-gam.txt")))
  }

  ## Initalize multicore
  sfInit(parallel=TRUE, cpus=4)
  sfClusterSetupRNG()
  sfExport("y", "X", "params")
  sfLibrary(coda)
  sfLibrary(here)

  ## create temporary progress file
  file.create(here("model-fit", "progress", "sim-gam.txt"))
 
  ## create temporary progress file and start timing
  file.create(here("model-fit", "progress", "sim-gam.txt"))
    start <- Sys.time()
  sink(here("model-fit", "progress", "sim-gam.txt"))
  print(paste("MCMC started at", start))
  sink()

  ## run MCMC
  out_splines <- sfLapply(1:4, parallelChains)
  
  ## end timing
  sink(here("model-fit", "progress", "sim-gam.txt"), append = TRUE)
  print(Sys.time() - start)
  sink()
  
  ## stop the computing cluster
  sfStop()
  
  save(out_splines, file=here("model-fit", "fit-gaussian-gam.RData"))
}
```


```{r}
## process spline model output
## initialze posterior sample variables
beta_post_splines <- array(0, dim=c(n_samples, df, d))
sigma2_post_splines <- rep(0, n_samples)
X_post_splines <- matrix(0,  n_samples, N-N_obs)

for(i in 1:n_chains){
  beta_post_splines[1:n_save + (i-1)*n_save, , ] <- out_splines[[i]]$beta
  sigma2_post_splines[1:n_save + (i-1)*n_save] <- out_splines[[i]]$sigma2
  X_post_splines[1:n_save + (i-1)*n_save, ] <- out_splines[[i]]$X
}
```



```{r, eval=TRUE}
out_splines <- convert_to_coda(out_splines)
```



```{r, eval=TRUE}
Rhat_splines <- make_gelman_rubin(out_splines)
layout(matrix(1:6, 3, 2))
hist(Rhat_splines[grepl("alpha", names(Rhat_splines))], main = "Rhat for alpha")
hist(Rhat_splines[grepl("beta", names(Rhat_splines))], main = "Rhat for beta")
hist(Rhat_splines[grepl("sigma2", names(Rhat_splines))], main = "Rhat for sigma2")
hist(Rhat_splines[grepl("X", names(Rhat_splines))], main = "Rhat for X")
hist(Rhat_splines, main="All parameters")
max(unlist(na.omit(Rhat_splines)))
```


```{r}
##
## evaluate predictive ability
##

CRPS_MVGP <- makeCRPS(X_post, X[(N_obs+1):N], n_save)
X_mean <- apply(X_post, 2, mean)
MSPE_MVGP <- (X_mean -  X[(N_obs+1):N])^2
MAE_MVGP <- abs(apply(X_post, 2, median) - X[(N_obs+1):N])
X_025 <- apply(X_post, 2, quantile, prob = 0.025)
X_975 <- apply(X_post, 2, quantile, prob = 0.975)
coverage_MVGP <- ( X[(N_obs+1):N] >= X_025) & ( X[(N_obs+1):N] <= X_975)

## Spline Model
CRPS_splines <- makeCRPS(X_post_splines, X[(N_obs+1):N], n_save)
X_mean_splines <- apply(X_post_splines, 2, mean)
MSPE_splines <- (X_mean_splines -  X[(N_obs+1):N])^2
MAE_splines  <- abs(apply(X_post_splines, 2, median) -   X[(N_obs+1):N])
X_025_splines <- apply(X_post_splines, 2, quantile, prob = 0.025)
X_975_splines <- apply(X_post_splines, 2, quantile, prob = 0.975)
coverage_splines <- ( X[(N_obs+1):N] >= X_025_splines) & ( X[(N_obs+1):N] <= X_975_splines)


## Random Forest
train <- data.frame(moisture=X[1:N_obs], y[1:N_obs, ])
test <- data.frame(y[(N_obs+1):N, ])
rf <- randomForest(moisture ~ ., data = train)
CRPS_rf <- makeCRPS(t(matrix(predict(rf, test, predict.all=TRUE)$individual,
                             N-N_obs, 500)), X[(N_obs+1):N], 500)
MSPE_rf <- (predict(rf, test) - X[(N_obs+1):N])^2
MAE_rf <- abs(predict(rf, test) - X[(N_obs+1):N])
rf_CI <- t(apply( predict(rf, test, predict.all=TRUE)$individual, 1,
                  function(x) {
                    quantile(x, c(0.025,0.975))
                  }))
coverage_rf <- ((X[(N_obs+1):N] >= rf_CI[, 1]) &
               (X[(N_obs+1):N] <= rf_CI[, 2]))

# CRPS_out <- cbind(CRPS_MVGP, CRPS_splines, MAE_rf)
# MSPE_out <- cbind(MSPE_MVGP, MSPE_splines, MSPE_rf)
# MAE_out <- cbind(MAE_MVGP, MAE_splines, MAE_rf)
# coverage_out <- cbind(coverage_MVGP, coverage_splines, coverage_rf)
# colnames(CRPS_out) <- c("MVGP", "GAM", "RF")
# colnames(MAE_out) <- c("MVGP", "GAM", "RF")
# colnames(MSPE_out) <- c("MVGP", "GAM", "RF")
# colnames(coverage_out) <- c("MVGP", "GAM", "RF")
CRPS_out <- cbind(CRPS_MVGP, CRPS_splines)
MSPE_out <- cbind(MSPE_MVGP, MSPE_splines)
MAE_out <- cbind(MAE_MVGP, MAE_splines)
coverage_out <- cbind(coverage_MVGP, coverage_splines)
colnames(CRPS_out) <- c("MVGP", "GAM")
colnames(MAE_out) <- c("MVGP", "GAM")
colnames(MSPE_out) <- c("MVGP", "GAM")
colnames(coverage_out) <- c("MVGP", "GAM")

CRPS <- data.frame(t(apply(CRPS_out, 2, mean)))
MSPE <- data.frame(t(apply(MSPE_out, 2, mean)))
MAE <- data.frame(t(apply(MAE_out, 2, mean)))
coverage <- data.frame(100/(N-N_obs)*t(apply(coverage_out, 2, sum)))

sim_results <- rbind(CRPS, MSPE, MAE, coverage)
rownames(sim_results) <- c("CRPS", "MSPE", "MAE", "95% CI coverage rates")
print(xtable(sim_results, digits=4), file=here("results", "sim-gaussian.tex"),
      floating=FALSE)
```

```{r}
kable(sim_results)
```


